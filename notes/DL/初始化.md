| constant          | ConstantFiller         | 使用一个常数（默认为0）初始化权值 |
| ----------------- | ---------------------- | --------------------------------- |
| gaussian          | GaussianFiller         | 使用高斯分布初始化权值            |
| positive_unitball | PositiveUnitballFiller |                                   |
| uniform           | UniformFiller          | 使用均为分布初始化权值            |
| xavier            | XavierFiller           | 使用xavier算法初始化权值          |
| msra              | MSRAFiller             |                                   |
| bilinear          | BilinearFiller         |                                   |

### constant初始化方法：

      它就是把权值或着偏置初始化为一个常数。

### uniform初始化方法

    它的作用就是把权值与偏置进行 均匀分布的初始化。用min 与 max 来控制它们的的上下限，默认为（0，1）.

### Gaussian 初始化

给定高斯函数的均值与标准差,生成高斯分布就可以了。

### positive_unitball 初始化

通俗一点，它干了点什么呢？即让每一个单元的输入的权值的和为 1. 例如吧，一个神经元有100个输入，这样的话，让这100个输入的权值的和为1. 源码中怎么实现的呢？ 首先给这100个权值赋值为在（0，1）之间的均匀分布，然后，每一个权值再除以它们的和就可以啦。

感觉这么做，可以有助于防止权值初始化过大，使激活函数（sigmoid函数）进入饱和区。所以呢，它应该比适合simgmoid形的激活函数。

它不需要参数去 控制。

### XavierFiller初始化：

对于这个初始化的方法，是有理论的。它来自这篇论文[《Understanding the difficulty of training deep feedforward neural networks》](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf)。在推导过程中，我们认为处于 tanh激活函数的线性区，所以呢，对于ReLU激活函数来说，XavierFiller初始化也是很适合啦。

如果不想看论文的话，可以看看 <https://zhuanlan.zhihu.com/p/22028079>，我觉得写的很棒，另外，<http://blog.csdn.net/shuzfan/article/details/51338178>可以作为补充。

它的思想就是让一个神经元的输入权重的（当反向传播时，就变为输出了）的方差等于：1 / 输入的个数；这样做的目的就是可以让信息可以在网络中均匀的分布一下。

对于权值的分布：是一个让均值为0，方差为1 / 输入的个数 的 均匀分布。

### MSRAFiller初始化方式

它与上面基本类似，它是基于[《Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification》](http://arxiv.org/pdf/1502.01852.pdf)来推导的，并且呢，它是基于激活函数为 ReLU函数哦，

对于权值的分布，是基于均值为0，方差为 2 /输入的个数 的高斯分布，这也是和上面的Xavier Filler不同的地方；它特别适合激活函数为 ReLU函数的啦。

### BilinearFiller初始化

对于它，要还没有怎么用到过，它常用在反卷积神经网络里的权值初始化；