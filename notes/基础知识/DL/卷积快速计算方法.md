## 高速卷积解析

### Some Prerequisites

#### FLOP/s

FLOP/s （**FL**oating **P**oint **O**perations per **S**econd），每秒的浮点运算数。具有更多浮点运算的操作自然会运行得会更慢，因此`FLOP/s`是比较性能的比较一致的方式。

#### 数据存储顺序以及行优先存储
对于一个四维张量 （比如`N*C*H*W`），实际在内存中的存取顺序与逻辑上的顺序是不一致的。在实际的内存中，四维的张量存储在一个一维的数组中。那么这两者之间的转换顺序就大大影响了我们的运算效率。

现在绝大多数的DL框架采用的都是**行优先**的存储方式。这种方式中，相邻行的数据会靠近在一起。当具有多个维度数据时，如果按照第一个维度进行遍历数据，相对于按照行来遍历，速度要慢很多。

比如，卷积运算中，数据的存储方式可以分为 `N*C*H*W`（pytorch）,`N*H*W*C`（caffe）等等。下图展示了不同方式下的内存中数据组织方式

![](https://tuchuang-1259359185.cos.ap-chengdu.myqcloud.com/bolgs/storage-order.png)



#### Halide
通常很多优化算法需要C甚至是汇编语言一起，这就使得代码理解起来极其困难。这里将采用 [Halide](https://halide-lang.org/) 语言来完成优化的操作。有关这门语言的介绍见于其他部分。

[相关介绍-zhihu](https://www.zhihu.com/question/294625837)

### 暴力计算

在讲如何优化之前首先讲一下，如果不优化，直接计算是怎么做的。

```python
'''
Convolve `input` with `kernel` to generate `output`
    input.shape = [input_channels, input_height, input_width]
    kernel.shape = [num_filters, input_channels, kernel_height, kernel_width]
    output.shape = [num_filters, output_height, output_width]
'''
for filter in 0..num_filters
    for channel in 0..input_channels
        for out_h in 0..output_height
            for out_w in 0..output_width
                for k_h in 0..kernel_height
                    for k_w in 0..kernel_width
                        output[filter, channel, out_h, out_h] +=
                            kernel[filter, channel, k_h, k_w] *
                            input[channel, out_h + k_h, out_w + k_w]
```


这是6个嵌套的循环（如果对多个输入的`batch`进行迭代，则为7）。还没有看到步幅，扩张或任何其他参数。如果使用这个计算方法，(输入大小为)，需要高达22秒！使用最优的编译器优化（如-O3或-Ofast），减少到2.2秒。但对于第一层来说，这仍然非常慢。

如果使用Caffe运行相同的层呢？在同一台PC上只用了18ms。超过了100倍的加速！

	>**瓶颈在哪儿，应该从哪里开始优化？**

最内层循环执行2次浮点运算（乘法和加法），对于我使用的大小，它执行了大约8516万次，即这种卷积需要1.7亿次浮点运算（MFLOP）。
根据英特尔的说法，我的CPU的峰值性能是每秒800亿FLOP，即它理论上可以在0.002秒内完成工作。显然，远不及那么明显，原始处理能力在这里已经绰绰有余了。没有实现理论峰值的原因是内存访问也需要时间 - 如果无法快速获取数据，则不足以快速处理数据。事实证明，上面存在大量嵌套的for循环会导致非常困难的数据访问模式，这些模式很难利用缓存。正如将看到的，整个讨论中反复出现的问题将是如何访问正在操作的数据，以及它与存储方式的关系。

### Speeding up the GEMM

#### Naive

### Caching

### Tiling

### Threading

### Unrolling

---

**来源:** [Anatomy of a High-Speed Convolution](https://sahnimanas.github.io/post/anatomy-of-a-high-performance-convolution/)
